# Phi-3.5-Law: M&A Contract Clause Extraction

**Phi-3.5-Law** is a dataset preparation tool designed to help create datasets for training language models to extract specific clauses from mergers and acquisitions (M&A) contracts. The datasets generated by this project are used to fine-tune the phi-3.5-mini-instruct model, available on Hugging Face ([Phi-3.5-Law Model](https://huggingface.co/DanielShaw98/phi-3.5-law)). The model is optimised to identify key clauses within M&A contracts, such as termination rights, indemnification, price adjustments, etc. and can be queried to find specific clause information.

## Overview

The project prepares the dataset by parsing M&A contracts, chunking the text into manageable sections, and using GPT-4o-mini (via OpenAI) to analyse the contract chunks to find specific clauses. The goal is to identify relevant clauses or return "nothing found" when no clause is detected. The output is stored in a JSON format that can be used for further analysis or to train the model.

This is a work in progress. More datasets are being created to improve the accuracy and reliability of the model. The ultimate goal is to create a tool that law firms can run locally, enabling them to securely input sensitive data without exposing it to third-party services. Eventually, the model could become powerful enough to help lawyers quickly navigate complex contracts by querying for specific clauses.

## File Structure

    src/

    |-- pdf_parser.py # Extracts the text and metadata from a contract PDF.

    |-- chunker.py # Chunks the extracted text into sections of ~2500 characters for analysis.
    
    |-- integrate.py # Combines pdf_parser and chunker for testing.
    
    |-- analyser.py # Makes API calls to OpenAI's GPT-4o-mini to find relevant clauses.
    
    |-- main.py # Combines all methods and generates JSON outputs for each query.
    
    data/
    
    |-- contracts/ # Sample M&A contracts in PDF format.
    
    |-- outputs/ # JSON output files created from main.py.
    
    outputs/
    
    |-- merge.js / merge2.js # Scripts for merging JSON files into one large dataset.
    
    |-- count.js # Counts relevant clauses found in the dataset.
    
    |-- reduce.js # Randomly removes half of the "nothing found" cases.
    
    |-- script.js # Formats the JSON file for conversation-style fine-tuning.  

## How It Works

-   PDF Parsing: pdf_parser.py extracts text and metadata from the provided contract PDFs.
    
-   Chunking: chunker.py breaks down the parsed text into manageable chunks (~2500 characters).
    
-   Clause Identification: analyser.py makes API calls to OpenAI, sending chunked text and a system message instructing the model to identify specific M&A clauses.
    
-   Output Generation: main.py processes the contract data with multiple queries and saves the results as JSON files in the /outputs folder.
    
-   Post-Processing: JavaScript scripts merge, count, reduce, and format the output files for further dataset preparation.
    

## Example Queries

The model is trained to identify a wide range of M&A contract clauses, including but not limited to:

-   Termination Rights and Conditions
    
-   Representations and Warranties
    
-   Indemnification
    
-   Material Adverse Change (MAC) Provisions
    
-   Purchase Price Adjustments
    
-   Non-Compete and Non-Solicitation Agreements
    
-   Confidentiality and Non-Disclosure Obligations
    
-   Escrow or Holdback Provisions
    
-   Dispute Resolution Mechanisms
    

## Installation

To set up the project locally:

1.    Clone the repository:  

    git clone https://github.com/your-username/data-prep.git
    cd data-prep  

2.   Install the necessary Python dependencies:  

    pip install -r requirements.txt
    
3.  Place your M&A contract PDF files in the /data/contracts folder.
    

## Usage

Run main.py to parse contracts, chunk text, and analyse clauses:

    python src/main.py

Specify the contract files and queries in main.py:

    queries = [
    
    "Review the provided text and identify all clauses related to termination rights and conditions.",
    
    "Identify all clauses related to indemnification.",
    
    ...
    
    ]

The JSON output will be saved to the /data/outputs/ directory.

## Future Use Case

Once fully developed, the model is intended to be run locally, making it suitable for law firms that need to process sensitive contracts without sending data to external services. The model could eventually assist lawyers in navigating complex contracts, finding specific clauses quickly and accurately, and supporting due diligence or contract review workflows.

## Contributors

-   [DanielShaw98](https://github.com/DanielShaw98) – Project lead and developer, created datasets and implemented model training for clause extraction.
    
-   [rayhanrsb](https://github.com/rayhanrsb) – Assisted throughout the entire project with dataset creation and model fine-tuning guidance.
